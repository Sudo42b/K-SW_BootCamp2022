{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Various Computer Vision Tasks\n",
    "\n",
    "<img src=\"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F13342055-3f47-45b0-8465-e0b62ab96315%2Fimage.png\">\n",
    "\n",
    "- Computer Vision에 활용되는 모델은 task에 따라 Classification, Detection, Segmentation 3가지로 나눌 수 있음\n",
    "\n",
    "## 1.1 Classification\n",
    "\n",
    "- 입력으로 주어진 이미지 안에 어떤 객체가 있는지에 따라 클래스(class)를 구분하는 행위\n",
    "- ex) 고양이의 사진을 보고 Cat, 강아지의 사진을 보고 Dog로 분류해주는 것\n",
    "\n",
    "<img src=\"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F9fa9beb2-c243-4ab6-acb3-8567183b10d5%2Fimage.png\">    \n",
    "    \n",
    "## 1.2 Object Detection\n",
    "\n",
    "- 객체(object)의 분류뿐만 아니라 해당 객체가 어느 위치에 있는지 bounding box를 통해 찾는 것 (Classification + Localization)\n",
    "- bounding box: 이미지에서 하나의 객체 전체를 포함하는 가장 작은 직사각형\n",
    "- 물체가 있는 영역의 위치정보를 bounding box로 찾고, bounding box 내에 존재하는 객체의 레이블(label)을 분류\n",
    "- 단순히 이미지에 레이블을 할당하는 이미지 인식(Image Recognition)보다 더 많고 세분화된 정보를 제공함\n",
    "  - ex) 자율주행 자동차에서 다른 자동차와 보행자를 찾을 때, 의료 분야에서 방사선 사진을 사용해 종양이나 위험한 조직을 찾을 때\n",
    "  \n",
    "<img src=\"https://wikidocs.net/images/page/163639/1_cats_and_person.jpg\">\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1038/0*zs2zJGdh92YUH98A.png\">\n",
    "\n",
    "## 1.3 Semantic Segmentation\n",
    "\n",
    "- 이미지에 있는 각 픽셀이 어느 클래스에 속하는지 예측하는 것 (Dense Prediction이라고도 불림)\n",
    "- 이미지의 모든 픽셀을 해당하는 클래스로 분류하는 것이 목적임\n",
    "- 같은 클래스인 객체가 여러 개 있다면 각각을 구별하지 않고 모두 동일한 요소로 간주함\n",
    "- 강아지 output 채널에서는 각 픽셀들에 대해 강아지에 포함되는 픽셀인지 아닌지, 사람 output 채널에서는 각 픽셀들이 사람에 포함되는 픽셀인지 아닌지를 판단함\n",
    "  - ex) 사진을 찍을 때 인물을 찾아내서 인물을 제외한 배경을 흐릿하게 만듦\n",
    "    \n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdJxoTd%2FbtqvGCuwDOc%2FJS8wb7Vvn4fJH1wTHFT9sk%2Fimg.png\">\n",
    "\n",
    "<img src=\"https://1.bp.blogspot.com/-r46T6fX9WLI/Xhr2x1BLeiI/AAAAAAAEp6s/dopxXegxMFoc8NAUk9SjcRAIC3Obh58HgCLcBGAsYHQ/s640/Semantic%2BSegmentation%2B2.png\">\n",
    "    \n",
    "\n",
    "## 1.4 Instance Segmentation\n",
    "- Semantic Segmentation과 달리, 픽셀 단위로 어떤 클래스인지 구분한 이후 동일한 클래스 내에서도 개별 개체(instance)를 구별\n",
    "- object가 겹쳤을 때 각각의 object를 구분해주지 못하는 Semantic Segmentation에서의 문제를 Instance Segmentation을 통해 해결할 수 있음\n",
    "\n",
    "<img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11263-019-01247-4/MediaObjects/11263_2019_1247_Fig4_HTML.png?as=webp\">\n",
    "    \n",
    "## 1.5 Classification VS Object Detection\n",
    "- 실제 이미지에는 클래스가 같거나 다른 종류의 instancce가 포함될 수 있음\n",
    "- Classification: 일반적으로 이미지 안에 하나의 객체가 있다고 가정하여 input에 대해 하나의 물체를 구분하는 작업\n",
    "- Object Detection: 물체를 구분함과 동시에, 각 객체를 둘러싸는 bounding box를 생성하여 그 객체가 어디에 있는지까지 구분하는 작업\n",
    "    \n",
    "<img src=\"https://wikidocs.net/images/page/163639/2_object_detection_vs_image_recognition.jpg\">\n",
    "    \n",
    "## 1.6 Sementic Segmentation VS Instasnce Segmentation\n",
    "- Semantic Segmenation: 객체를 segmentation하되 같은 클래스에 속하는 객체들에 대해 서로 구분지을 수 없다는 단점이 있음\n",
    "- Instance Segmentation: 같은 클래스여도 서로 다른 instance로 구분해줌\n",
    "\n",
    "<img src=\"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Faa245159-a1f4-47f3-8e30-4f48ec916543%2Fimage.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Object Detection 종류\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/163640/od_history.png\">\n",
    "\n",
    "- Object Detector 모델은 크게 Two-Stage Detector와 Single-Stage Detector로 나눌 수 있음\n",
    "\n",
    "## 2.1 What is Two-Stage Detector?\n",
    "    \n",
    "<img src=\"https://wikidocs.net/images/page/163640/Fig_1_stage.png\">\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/163639/3_rpn.jpg\">\n",
    "    \n",
    "- RCNN, Fast R-CNN, Faster-RCNN 등 원하는 특징추출(Feature Extractor) 이후 객체(object)의 위치를 RPN(Regional Proposal Network)이라는 네트워크가 한번 더 연산 후 위치(Bounding Box)와 클래스(class)를 총 2번에 걸쳐 예측\n",
    "  - Region Proposal Network: input으로 들어온 이미지에서 물체가 있을만한 영역을 빠르게 찾아내는 알고리즘 (객체의 위치를 찾는 Localization 문제임)\n",
    "  - 즉, **물체가 있을 것이라 판단되는 영역을 RPN이라는 네트워크로 bounding box로 대략적으로 찾은 후에 그 영역 내에서 Classification을 진행하여 결과를 도출함**\n",
    "- Two-Stage Detector은 객체를 검출하는 정확도 측면에서는 좋은 성능을 냈지만 예측 속도가 느려서 실시간 탐지를 하고자 하는 어플리케이션에서는 부적합하여 이러한 속도 문제를 해결하기 위해 Regional Proposal과 Classification을 동시에 하는 One(Single)-Stage Detector가 제안 됨\n",
    "  - 장점: 정확도가 높고 다양한 instance기반 작업에 확장이 용이함\n",
    "  - 단점: 종단간 훈련 불가능하고 속도가 느림\n",
    "        \n",
    "## 2.2 What is Single-Stage Detector?\n",
    "    \n",
    "<img src=\"https://wikidocs.net/images/page/163640/Fig_2_stage.png\">\n",
    "    \n",
    "- YOLO(You Only Look Once), SSD(Single Shot Multibox Detector), RetinaNet 등\n",
    "- Two-Stage Detector과 달리 FE(Feature Extractor)이후, 객체의 class와 bounding box를 한번에 예측\n",
    "- 이미지 내의 모든 위치를 object의 잠재영역으로 보고 각 후보영역에 대해 class 예측\n",
    "### YOLO(You Only Look Once)모델의 구조\n",
    "- 입력 이미지가 Backbone(FE)네트워크를 통과하게 되면 결과를 일정영역의 Grid Cell이라고 하는 영역에 초기에 설정한 Grid cell마다 Class Probability, bounding box 좌표, Confidence값을 얻어냄\n",
    "  <img src=\"https://i.stack.imgur.com/aUcNf.jpg\">\n",
    "- 최종적으로 나온 Grid Cell은 Receptive Field와 관련이 있음\n",
    "\n",
    "<img src=\"https://theaisummer.com/static/490be17ee7f19b78003c3fdf5a6bbafc/83b75/receptive-field-in-convolutional-networks.png\" width=347, height=272>\n",
    "<img src=\"https://i.stack.imgur.com/fVbPv.png\">\n",
    "\n",
    "  - 장점: 처리과정이 간단하고 수행 속도가 빠름\n",
    "  - 단점: 불규칙한 모양의 객체나 작은 객체에서는 상대적으로 정확도가 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Modern Object Detection Architecture\n",
    "\n",
    "- RCNN (2013)\n",
    "  \n",
    "  <img src=\"https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/OD-ch1img08.png?raw=true\">\n",
    "  \n",
    "  - [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n",
    "  - 물체 검출에 사용된 BoundingBox를 찾고자 Selective Search를 사용하였고 시간이 많이 걸린다는 특징을 가지고 있음\n",
    "\n",
    "    <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FTqB2Y%2FbtrmYbGoOu6%2F58rxSTlqPl4EhqgXsAs6uK%2Fimg.png\" width=\"50%\" height=\"50%\">\n",
    "  \n",
    "  - 매우 높은 성능의 탐지가 가능하지만, 복잡한 아키텍처 및 학습 프로세스로 인해 객체탐지 시간이 매우 오래 걸림(CPU)\n",
    "  \n",
    "  <img src=\"https://seongkyun.github.io/assets/post_img/papers/2019-01-06-Object_detection/fig2.PNG\"  width=\"70%\" height=\"70%\">\n",
    "  \n",
    "  1. Hypothesize Bounding Boxes (Proposals)\n",
    "    - Image로부터 Object가 존재할 적절한 위치에 Bounding Box Proposal (Selective Search) 2000개의 Proposal이 생성됨.\n",
    "  2. Resampling pixels / features for each boxes\n",
    "    - 모든 Proposal을 Crop 후 동일한 크기로 만듦 (224x224x3)\n",
    "  3. Classifier / Bounding Box Regressor\n",
    "    - 위의 입력을 Classifier와 Bounding Box Regressor로 처리\n",
    "    - 하지만 모든 Proposal에 대해 CNN을 거쳐야 하므로 연산량이 매우 많은 단점\n",
    "\n",
    "- SPP Net (2014)\n",
    "  - [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)\n",
    "  - RCNN의 문제를 Selective search로 해결하려 했지만, bounding box의 크기가 제각각인 문제가 있어서 FC Input에 고정된 사이즈로 제공하기 위한 방법 제안\n",
    "\n",
    "    <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FuQeYg%2FbtrsrmXz5Dm%2F1rjkB6Ak1FmSPfU79XJuqk%2Fimg.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "  - SPP은 RCNN에서 conv layer와 fc layer사이에 위치하여 서로 다른 feature map에 투영된 이미지를 고정된 값으로 풀링\n",
    "  - SPP를 이용해 RCNN에 비해 실행시간을 매우 단축시킴\n",
    "\n",
    "- [Fast RCNN](https://arxiv.org/abs/1504.08083) (2015)\n",
    "  - Fast R-CNN \n",
    "  - SPP layer를 ROI pooling으로 바꿔서 7x7 layer 1개로 해결\n",
    "  - SVM을 softmax로 대체하여 Classification 과 Regression Loss를 함께 반영한 Multi task Loss 사용\n",
    "  - ROI Pooling을 이용해 SPP보다 간단하고, RCNN에 비해 수행시간을 많이 줄임\n",
    "\n",
    "    <img src=\"https://seongkyun.github.io/assets/post_img/papers/2019-01-06-Object_detection/fig3.PNG\" width=\"70%\" height=\"70%\">\n",
    "  \n",
    "\n",
    "- Fater RCNN(2015)\n",
    "  - Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (https://arxiv.org/abs/1506.01497)\n",
    "  - RPN(Region proposal network) + Fast RCNN 방식\n",
    "  - Selective Search를 대체하기 위한 Region Proposal Network구현\n",
    "  - RPN도 학습시켜서 전체를 end-to-end로 학습 가능 (GPU사용 가능)\n",
    "  - Region Proposal를 위해 Object가 있는지 없는지의 후보 Box인 Anchor Box 개념 사용\n",
    "  - Anchor Box를 도입해 FastRCNN에 비해 정확도를 높이고 속도를 향상시킴\n",
    "\n",
    "    <img src=\"https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/OD-ch1img10-2.png?raw=true\">\n",
    "   \n",
    "\n",
    "- SSD (2015)\n",
    "  - SSD: Single Shot MultiBox Detector (https://arxiv.org/abs/1512.02325)\n",
    "  - Faster-RCNN은 region proposal과 anchor box를 이용한 검출의 2단계를 걸치는 과정에서 시간이 필요해 real-time(20~30 fps)으로는 어려움\n",
    "  - SSD는 Feature map의 size를 조정하고, 동시에 앵커박스를 같이 적용함으로써 1 shot으로 물체 검출이 가능\n",
    "  - real-time으로 사용할 정도의 성능을 갖춤 (30~40 fps)\n",
    "  - 작은 이미지의 경우에 잘 인식하지 못하는 경우가 생겨서 data augmentation을 통해 mAP를 63에서 74로 비약적으로 높임\n",
    "\n",
    "    <img src=\"https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/OD-ch1img12.PNG?raw=true\">\n",
    "   \n",
    "- RetinaNet (2017)\n",
    "  - Focal Loss for Dense Object Detection (https://arxiv.org/abs/1708.02002)\n",
    "  - RetinaNet이전에는 1-shot detection과 2-shot detection의 차이가 극명하게 나뉘어 속도를 선택하면 정확도를 trade-off 할 수 밖에 없는 상황\n",
    "  - RetinaNet은 Focal Loss라는 개념의 도입과 FPN 덕분에 기존 모델들보다 정확도도 높고 속도도 여타 1-shot detector와 비견되는 모델\n",
    "  - Detection에선 검출하고 싶은 물체와 (foreground object) 검출할 필요가 없는 배경 물체들이 있는데 (background object) 배경 물체의 숫자가 매우 많을 경우 배경 Loss를 적게 하더라도 숫자에 압도되어 배경의 Loss의 총합을 학습해버림 (예를 들어, 숲을 배경으로 하는 사람을 검출해야하는데 배경의 나무가 100개나 되다보니 사람의 특징이 아닌 나무가 있는 배경을 학습해버림)\n",
    "  - Focal Loss는 이런 문제를 기존의 crossentropy 함수에서 (1-sig)을 제곱하여 background object의 loss를 현저히 줄여버리는 방법으로 loss를 변동시켜 해결\n",
    "  - Focal Loss를 통해 검출하고자 하는 물체와 관련이 없는 background object들은 학습에 영향을 주지 않게 되고, 학습의 다양성이 더 넓어짐 (작은 물체, 큰 물체에 구애받지 않고 검출할 수 있게됨)\n",
    "  - 실제로 RetinaNet은 object proposal을 2000개나 실시하여 이를 확인\n",
    "\n",
    "    <img src=\"https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/OD-ch1img13.PNG?raw=true\">\n",
    "   \n",
    "\n",
    "- Mask R-CNN (2018)\n",
    "  - Mask R-CNN (https://arxiv.org/pdf/1703.06870.pdf)\n",
    "\n",
    "- YOLO (2016)\n",
    "  - YOLO는 v1, Yolo-v2(2017), Yolo-v3(2018)의 순서로 발전하였는데, v1은 정확도가 너무 낮은 문제가 있었고 이 문제는 v2까지 이어짐, 현재는 Yolo-v7까지 나왔음\n",
    "  - 엔지니어링적으로 보완한 v3는 v2보다 살짝 속도는 떨어지더라도 정확도를 대폭 높인 모델\n",
    "  - RetinaNet과 마찬가지로 FPN을 도입해 정확도를 높임\n",
    "  - RetinaNet에 비하면 정확도는 4mAP정도 떨어지지만, 속도는 더 빠르다는 장점\n",
    "  - [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)\n",
    "  - [YOLO9000: Better, Faster, Stronger, 2017](https://arxiv.org/abs/1612.08242)\n",
    "  - [YOLOv: An Incremental Improvement](https://arxiv.org/abs/1804.02767)\n",
    "  - \n",
    "    <img src=\"https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/OD-ch1img11.png?raw=true\">\n",
    "  \n",
    "  - YOLO v1의 특징\n",
    "    - YOLO의 접근법은 객체 탐지 모델의 FPS를 높였지만 너무나도 작은 물체에 대해서는 탐지가 잘 되지 않는다는 단점\n",
    "      - 이유는 Loss Function에서 바운딩 박스 후보군을 선정할 때 사물이 큰 객체는 바운딩 박스간의 IOU의 값이 크게 차이나기 때문에 적절한 후보군을 선택할 수 있는 반면, 작은 사물에 대해서는 약간의 차이가 IOU의 결과값을 뒤짚을 수 있기 때문\n",
    "      - 이 문제는 YOLO의 후속 버전에서 어느 정도 개선되었다.\n",
    "\n",
    "- RefineDet (2018)\n",
    "  - Single-Shot Refinement Neural Network for Object Detection (https://arxiv.org/pdf/1711.06897.pdf)\n",
    "\n",
    "- M2Det (2019)\n",
    "  - M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network (https://arxiv.org/pdf/1811.04533.pdf)\n",
    "\n",
    "- EfficientDet (2019)\n",
    "  - EfficientDet: Scalable and Efficient Object Detection (https://arxiv.org/pdf/1911.09070v1.pdf)\n",
    "\n",
    "- YOLOv4 (2020)\n",
    "  - YOLOv4: Optimal Speed and Accuracy of Object Detection (https://arxiv.org/pdf/2004.10934v1.pdf)\n",
    "  - YOLOv3에 비해 AP, FPS가 각각 10%, 12% 증가\n",
    "  - YOLOv3와 다른 개발자인 AlexeyBochkousky가 발표\n",
    "  - v3에서 다양한 딥러닝 기법(WRC, CSP ...) 등을 사용해 성능을 향상시킴\n",
    "  - CSPNet 기반의 backbone(CSPDarkNet53)을 설계하여 사용\n",
    "\n",
    "- YOLOv5 (2020)\n",
    "  - YOLOv4에 비해 낮은 용량과 빠른 속도 (성능은 비슷)\n",
    "  - YOLOv4와 같은 CSPNet 기반의 backbone을 설계하여 사용\n",
    "  - YOLOv3를 PyTorch로 implementation한 GlennJocher가 발표\n",
    "  - Darknet이 아닌 PyTorch 구현이기 때문에, 이전 버전들과 다르다고 할 수 있음\n",
    "\n",
    "- 이후\n",
    "  - 수 많은 YOLO 버전들이 탄생\n",
    "  - Object Detection 분야의 논문들이 계속해서 나오고 있음\n",
    "\n",
    "<img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11263-019-01247-4/MediaObjects/11263_2019_1247_Fig11_HTML.png?as=webp\">\n",
    "\n",
    "## 2.3 YOLO(You Only Look Once)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*bSLNlG7crv-p-m4LVYYk3Q.png\" width=400>\n",
    "\n",
    "- 가장 빠른 객체 검출 알고리즘 중 하나\n",
    "- Single-Stage Detection으로 기존의 detection 모델들에 비해 처리과정이 간단하기 때문에 학습과 예측의 속도가 빠름\n",
    "- 모든 학습 과정이 이미지 전체를 통해 일어나기 때문에 단일 대상의 특징뿐 아니라 이미지 전체의 맥락을 학습하게 됨\n",
    "- 대상의 일반적인 특징을 학습하기 때문에 다른 영역으로의 확장에서도 뛰어난 성능을 보임\n",
    "\n",
    "### 2.3.1 YOLO v1\n",
    "- 빠른 Detection 시간. 그러나 낮은 정확도 (그 뒤 SSD가 더 좋은 성능과 속도를 보였음)\n",
    "- 기존의 객체 탐지는 single window나 regional proposal methods 등을 통해 바운딩 박스를 잡은 후 탐지된 바운딩 박스에 대해 분류를 수행하는 two-stage detection였음<br>\n",
    " -> 파이프라인이 복잡하기 때문에 학습 및 예측, 최적화 느려진다는 단점 존재\n",
    "- YOLO v1은 하나의 컨볼루션 네트워크를 통해 대상의 위치와 클래스를 한번에 예측 가능한 one-stage detection임\n",
    "- 테두리 상자 조정(Bounding Box Coordinate)과 분류(Classification)을 동일 신경망 구조를 통해 동시에 실행하는 통합인식(Unified Detection)을 구현하는 것이 큰 특징\n",
    "  1. 이미지를 $S$ x $S$ 의 그리드로 분할\n",
    "  2. 이미지 전체를 신경망에 넣고 특징 추출을 통해 예측 텐서(Prediction Tensor) 생성. (예측 텐서는 그리드 별 테두리 상자 정보, 신뢰 점수, 분류 클래스 확률을 포함)\n",
    "  3. 그리드 별 예측 정보를 바탕으로 테두리 상자 조정 및 분류 작업 수행\n",
    "  4. 각각의 Grid cell은 B개의 바운딩 박스와 각 바운딩 박스에 대한 Confidence-Score를 가짐\n",
    "    - Confidence-Score: $Pr(Object) * IOU$\n",
    "  5. 각각의 Grid cell은 C개의 Conditional Class Probability를 가짐\n",
    "  6. 각각의 바운딩 박스는 $x, y, w, h, confidence$를 지님.\n",
    "    - $(x, y)$ : Bounding box의 중심점을 의미, grid cell 범위에 대한 상대 값\n",
    "    - ex. $x$가 grid cell 가장 왼쪽, $y$가 grid cell 중간에 있다면 $x=0, y=0.5$\n",
    "\n",
    "<img src=\"https://jhgan00.github.io/assets/img/yolov1-1.png\">\n",
    "\n",
    "- 한계\n",
    "  - 하나의 cell에서 하나의 object만 예측하므로 물체들이 겹쳐있으면 제대로 된 예측이 어려움\n",
    "  - 물체가 작을수록 정확도가 감소함 \n",
    "  - 바운딩박스 형태가 data를 통해 학습되므로 새로운 형태의 바운딩박스인 경우 정확히 예측하지 못함\n",
    "\n",
    "### 2.3.2 YOLO v2\n",
    "\n",
    "- 수행시간과 성능 모두 개선 (SSD보다 빠르지만 성능은 아주 조금 개선됨)\n",
    "- YOLO v1 모델의 단점을 개선하여 검출율을 높이고 localization error를 줄임\n",
    "- 성능 개선을 위하여 앵커 박스(Anchor Box)라는 개념을 도입함. 이전에는 cell 중심 detection이었다면 이제는 anchor 중심 detection을 수행\n",
    "- 앵커박스\n",
    "  - 이미지에서 다양한 형태의 object를 detection하기 위한 미리 정해진 크기와 비율을 가진 bounding box\n",
    "  - 단, 앵커박스의 개수와 형태(width, height등)은 사용자가 임의로 지정 \n",
    "  - 기존 YOLO와 다르게(objectness, x, y, w, h, c, x, y, w, h, c), YOLO v2에서는 Anchor Box의 개수만큼(objectness, x, y, w, h, c, objectness, x, y, w, h, c, ...) 클래스와 객체의 유무를 예측함. 즉, 한 개의 cell에서 여러개의 앵커를 통해 개별 cell에서 여러 개의 object detection을 수행\n",
    "  - YOLO v1은 '객체의 정확한 위치'를 예측해야 했으나, 앵커 박스를 사용하면 '각 앵커박스와 객체와의 차이 값'을 예측하면 되기 때문에 해결해야 될 문제의 난이도가 낮아져 성능이 상승됨\n",
    "  - 탐지할 수 있는 객체 개수 증가 (mAP는 감소하지만 recall이 높아져서 예측을 더 많이 할 수 있음)\n",
    "    - YOLO v1에서 탐지할 수 있는 최대 객체 개수 : 49개\n",
    "    - YOLO v2에서 탐지할 수 있는 최대 객체 개수 : 1000개 이상\n",
    "\n",
    "  <img src=\"https://kr.mathworks.com/help/vision/ug/ssd_detection.png\">\n",
    "\n",
    "\n",
    "### 2.3.3 YOLO v3\n",
    "\n",
    "- 수행시간은 조금 느려졌으나 성능은 대폭 개선\n",
    "- 각각 3개의 서로 다른 크기의 scale을 가진 앵커박스를 이용하여 작은 물체를 잘 못찾는다는 문제점을 해소\n",
    "- 실제 어떤 물체가 사람이면서 동시에 여성일 수도 있음. 즉, 한 물체에 따라 multi class label을 가질 수 있는데, v3에서는 이것이 가능하도록 마지막에 각 class별로 binary classification을 함\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/46951365/91930634-53dbc380-ed1c-11ea-9a3d-5adf7c028d0f.png\" width=800>\n",
    "\n",
    "### 2.3.4 YOLO v4\n",
    "\n",
    "- 정확도의 문제로 실제 YOLO만 단독으로 쓰이는 경우가 매우 적음을 지적하고 빠르면서도 정확한 모델을 만들기 위해 v4를 개발\n",
    "- GPU가 한개만 있어도 사용할 수 있도록 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c585f91d3623973be3accc48b0d5e967ce904a396a0f0c8bda7b100d8b60333f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
